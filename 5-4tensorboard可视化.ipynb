{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:469: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\anaconda\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:470: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\anaconda\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:471: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\anaconda\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:472: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\anaconda\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:473: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\anaconda\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:476: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.contrib.tensorboard.plugins import projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "Iter0,Testing Accuaracy0.29,Training Accuracy0.2797091,Learning Rate0.003\n",
      "Iter1,Testing Accuaracy0.4726,Training Accuracy0.46254545,Learning Rate0.00285\n",
      "Iter2,Testing Accuaracy0.5456,Training Accuracy0.5410182,Learning Rate0.0027075\n",
      "Iter3,Testing Accuaracy0.6006,Training Accuracy0.5958,Learning Rate0.002572125\n",
      "Iter4,Testing Accuaracy0.6242,Training Accuracy0.6248364,Learning Rate0.0024435187\n",
      "Iter5,Testing Accuaracy0.6477,Training Accuracy0.64585453,Learning Rate0.0023213427\n",
      "Iter6,Testing Accuaracy0.655,Training Accuracy0.6513636,Learning Rate0.0022052757\n",
      "Iter7,Testing Accuaracy0.6671,Training Accuracy0.6635818,Learning Rate0.002095012\n",
      "Iter8,Testing Accuaracy0.6846,Training Accuracy0.68316364,Learning Rate0.0019902613\n",
      "Iter9,Testing Accuaracy0.7332,Training Accuracy0.73236364,Learning Rate0.0018907483\n",
      "Iter10,Testing Accuaracy0.7704,Training Accuracy0.76525456,Learning Rate0.0017962108\n",
      "Iter11,Testing Accuaracy0.7803,Training Accuracy0.7717818,Learning Rate0.0017064003\n",
      "Iter12,Testing Accuaracy0.7819,Training Accuracy0.7731818,Learning Rate0.0016210802\n",
      "Iter13,Testing Accuaracy0.7857,Training Accuracy0.78058183,Learning Rate0.0015400263\n",
      "Iter14,Testing Accuaracy0.7936,Training Accuracy0.7886364,Learning Rate0.001463025\n",
      "Iter15,Testing Accuaracy0.7972,Training Accuracy0.79254544,Learning Rate0.0013898737\n",
      "Iter16,Testing Accuaracy0.7974,Training Accuracy0.7941091,Learning Rate0.00132038\n",
      "Iter17,Testing Accuaracy0.8108,Training Accuracy0.8062909,Learning Rate0.001254361\n",
      "Iter18,Testing Accuaracy0.8278,Training Accuracy0.8227636,Learning Rate0.001191643\n",
      "Iter19,Testing Accuaracy0.8458,Training Accuracy0.8368,Learning Rate0.0011320608\n",
      "Iter20,Testing Accuaracy0.85,Training Accuracy0.8411273,Learning Rate0.0010754578\n",
      "Iter21,Testing Accuaracy0.8526,Training Accuracy0.8414364,Learning Rate0.0010216848\n",
      "Iter22,Testing Accuaracy0.8515,Training Accuracy0.83976364,Learning Rate0.00097060064\n",
      "Iter23,Testing Accuaracy0.85,Training Accuracy0.8364546,Learning Rate0.0009220706\n",
      "Iter24,Testing Accuaracy0.8487,Training Accuracy0.83434546,Learning Rate0.0008759671\n",
      "Iter25,Testing Accuaracy0.8498,Training Accuracy0.8343818,Learning Rate0.00083216873\n",
      "Iter26,Testing Accuaracy0.8518,Training Accuracy0.83723634,Learning Rate0.00079056027\n",
      "Iter27,Testing Accuaracy0.8572,Training Accuracy0.8419818,Learning Rate0.00075103226\n",
      "Iter28,Testing Accuaracy0.8624,Training Accuracy0.84729093,Learning Rate0.00071348064\n",
      "Iter29,Testing Accuaracy0.8674,Training Accuracy0.8528,Learning Rate0.0006778066\n",
      "Iter30,Testing Accuaracy0.8694,Training Accuracy0.8556,Learning Rate0.0006439163\n",
      "Iter31,Testing Accuaracy0.8706,Training Accuracy0.8563273,Learning Rate0.0006117205\n",
      "Iter32,Testing Accuaracy0.8702,Training Accuracy0.8569091,Learning Rate0.00058113446\n",
      "Iter33,Testing Accuaracy0.8695,Training Accuracy0.8571636,Learning Rate0.0005520777\n",
      "Iter34,Testing Accuaracy0.8707,Training Accuracy0.85718185,Learning Rate0.00052447384\n",
      "Iter35,Testing Accuaracy0.8715,Training Accuracy0.8568182,Learning Rate0.0004982501\n",
      "Iter36,Testing Accuaracy0.8712,Training Accuracy0.8571636,Learning Rate0.00047333766\n",
      "Iter37,Testing Accuaracy0.871,Training Accuracy0.8575091,Learning Rate0.00044967077\n",
      "Iter38,Testing Accuaracy0.8727,Training Accuracy0.8589454,Learning Rate0.00042718722\n",
      "Iter39,Testing Accuaracy0.8735,Training Accuracy0.86112726,Learning Rate0.00040582786\n",
      "Iter40,Testing Accuaracy0.8753,Training Accuracy0.8635273,Learning Rate0.00038553646\n",
      "Iter41,Testing Accuaracy0.877,Training Accuracy0.86596364,Learning Rate0.00036625966\n",
      "Iter42,Testing Accuaracy0.8784,Training Accuracy0.86792725,Learning Rate0.00034794665\n",
      "Iter43,Testing Accuaracy0.8802,Training Accuracy0.8701636,Learning Rate0.00033054934\n",
      "Iter44,Testing Accuaracy0.8829,Training Accuracy0.87316364,Learning Rate0.00031402186\n",
      "Iter45,Testing Accuaracy0.8851,Training Accuracy0.8745273,Learning Rate0.00029832078\n",
      "Iter46,Testing Accuaracy0.8846,Training Accuracy0.8751091,Learning Rate0.00028340472\n",
      "Iter47,Testing Accuaracy0.8852,Training Accuracy0.87587273,Learning Rate0.0002692345\n",
      "Iter48,Testing Accuaracy0.885,Training Accuracy0.87601817,Learning Rate0.00025577276\n",
      "Iter49,Testing Accuaracy0.8842,Training Accuracy0.87652725,Learning Rate0.00024298413\n",
      "Iter50,Testing Accuaracy0.8841,Training Accuracy0.87701815,Learning Rate0.00023083492\n",
      "Iter51,Testing Accuaracy0.8847,Training Accuracy0.87749094,Learning Rate0.00021929319\n",
      "Iter52,Testing Accuaracy0.885,Training Accuracy0.87783635,Learning Rate0.00020832852\n",
      "Iter53,Testing Accuaracy0.8854,Training Accuracy0.8783091,Learning Rate0.0001979121\n",
      "Iter54,Testing Accuaracy0.8859,Training Accuracy0.87894547,Learning Rate0.0001880165\n",
      "Iter55,Testing Accuaracy0.8868,Training Accuracy0.87947273,Learning Rate0.00017861566\n",
      "Iter56,Testing Accuaracy0.8875,Training Accuracy0.87994546,Learning Rate0.00016968488\n",
      "Iter57,Testing Accuaracy0.8881,Training Accuracy0.88083637,Learning Rate0.00016120063\n",
      "Iter58,Testing Accuaracy0.8879,Training Accuracy0.88094544,Learning Rate0.0001531406\n",
      "Iter59,Testing Accuaracy0.8885,Training Accuracy0.88152725,Learning Rate0.00014548357\n",
      "Iter60,Testing Accuaracy0.8889,Training Accuracy0.8820364,Learning Rate0.00013820939\n",
      "Iter61,Testing Accuaracy0.8894,Training Accuracy0.8822909,Learning Rate0.00013129893\n",
      "Iter62,Testing Accuaracy0.8898,Training Accuracy0.88234544,Learning Rate0.00012473398\n",
      "Iter63,Testing Accuaracy0.8904,Training Accuracy0.8826182,Learning Rate0.00011849728\n",
      "Iter64,Testing Accuaracy0.8909,Training Accuracy0.8829455,Learning Rate0.000112572416\n",
      "Iter65,Testing Accuaracy0.8909,Training Accuracy0.8826727,Learning Rate0.0001069438\n",
      "Iter66,Testing Accuaracy0.891,Training Accuracy0.8827091,Learning Rate0.000101596605\n",
      "Iter67,Testing Accuaracy0.8912,Training Accuracy0.88283634,Learning Rate9.651678e-05\n",
      "Iter68,Testing Accuaracy0.8915,Training Accuracy0.88305455,Learning Rate9.169094e-05\n",
      "Iter69,Testing Accuaracy0.8921,Training Accuracy0.8831091,Learning Rate8.710639e-05\n",
      "Iter70,Testing Accuaracy0.8919,Training Accuracy0.8831818,Learning Rate8.2751074e-05\n",
      "Iter71,Testing Accuaracy0.892,Training Accuracy0.8832909,Learning Rate7.861352e-05\n",
      "Iter72,Testing Accuaracy0.8917,Training Accuracy0.8832545,Learning Rate7.4682845e-05\n",
      "Iter73,Testing Accuaracy0.8922,Training Accuracy0.88330907,Learning Rate7.09487e-05\n",
      "Iter74,Testing Accuaracy0.8922,Training Accuracy0.88332725,Learning Rate6.740126e-05\n",
      "Iter75,Testing Accuaracy0.8926,Training Accuracy0.8832727,Learning Rate6.40312e-05\n",
      "Iter76,Testing Accuaracy0.893,Training Accuracy0.88338184,Learning Rate6.0829643e-05\n",
      "Iter77,Testing Accuaracy0.8926,Training Accuracy0.88363636,Learning Rate5.778816e-05\n",
      "Iter78,Testing Accuaracy0.8929,Training Accuracy0.88385457,Learning Rate5.4898752e-05\n",
      "Iter79,Testing Accuaracy0.8932,Training Accuracy0.8839273,Learning Rate5.2153813e-05\n",
      "Iter80,Testing Accuaracy0.893,Training Accuracy0.8841818,Learning Rate4.9546125e-05\n",
      "Iter81,Testing Accuaracy0.8933,Training Accuracy0.88427275,Learning Rate4.7068817e-05\n",
      "Iter82,Testing Accuaracy0.8933,Training Accuracy0.88454545,Learning Rate4.4715376e-05\n",
      "Iter83,Testing Accuaracy0.8933,Training Accuracy0.88487273,Learning Rate4.247961e-05\n",
      "Iter84,Testing Accuaracy0.8934,Training Accuracy0.88525456,Learning Rate4.0355626e-05\n",
      "Iter85,Testing Accuaracy0.8934,Training Accuracy0.88543636,Learning Rate3.8337846e-05\n",
      "Iter86,Testing Accuaracy0.8937,Training Accuracy0.8854909,Learning Rate3.642095e-05\n",
      "Iter87,Testing Accuaracy0.8938,Training Accuracy0.8855636,Learning Rate3.4599907e-05\n",
      "Iter88,Testing Accuaracy0.8937,Training Accuracy0.88576365,Learning Rate3.286991e-05\n",
      "Iter89,Testing Accuaracy0.8936,Training Accuracy0.8858909,Learning Rate3.1226416e-05\n",
      "Iter90,Testing Accuaracy0.8937,Training Accuracy0.88592726,Learning Rate2.9665094e-05\n",
      "Iter91,Testing Accuaracy0.8939,Training Accuracy0.88592726,Learning Rate2.818184e-05\n",
      "Iter92,Testing Accuaracy0.894,Training Accuracy0.88601816,Learning Rate2.6772748e-05\n",
      "Iter93,Testing Accuaracy0.894,Training Accuracy0.88607275,Learning Rate2.543411e-05\n",
      "Iter94,Testing Accuaracy0.894,Training Accuracy0.88618183,Learning Rate2.4162404e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter95,Testing Accuaracy0.8942,Training Accuracy0.88618183,Learning Rate2.2954284e-05\n",
      "Iter96,Testing Accuaracy0.8942,Training Accuracy0.8863091,Learning Rate2.180657e-05\n",
      "Iter97,Testing Accuaracy0.8942,Training Accuracy0.8863636,Learning Rate2.0716241e-05\n",
      "Iter98,Testing Accuaracy0.8941,Training Accuracy0.8863636,Learning Rate1.968043e-05\n",
      "Iter99,Testing Accuaracy0.8944,Training Accuracy0.8864727,Learning Rate1.8696408e-05\n",
      "Iter100,Testing Accuaracy0.8944,Training Accuracy0.8865091,Learning Rate1.7761588e-05\n",
      "Iter101,Testing Accuaracy0.8945,Training Accuracy0.8865091,Learning Rate1.6873508e-05\n",
      "Iter102,Testing Accuaracy0.8945,Training Accuracy0.8864727,Learning Rate1.6029833e-05\n",
      "Iter103,Testing Accuaracy0.8944,Training Accuracy0.8865091,Learning Rate1.5228341e-05\n",
      "Iter104,Testing Accuaracy0.8944,Training Accuracy0.88656366,Learning Rate1.4466924e-05\n",
      "Iter105,Testing Accuaracy0.8945,Training Accuracy0.8865091,Learning Rate1.3743578e-05\n",
      "Iter106,Testing Accuaracy0.8945,Training Accuracy0.88658184,Learning Rate1.3056399e-05\n",
      "Iter107,Testing Accuaracy0.8946,Training Accuracy0.88674545,Learning Rate1.2403579e-05\n",
      "Iter108,Testing Accuaracy0.8945,Training Accuracy0.8868545,Learning Rate1.17834e-05\n",
      "Iter109,Testing Accuaracy0.8943,Training Accuracy0.8868182,Learning Rate1.119423e-05\n",
      "Iter110,Testing Accuaracy0.8943,Training Accuracy0.8868727,Learning Rate1.0634519e-05\n",
      "Iter111,Testing Accuaracy0.8943,Training Accuracy0.8869454,Learning Rate1.0102793e-05\n",
      "Iter112,Testing Accuaracy0.8942,Training Accuracy0.88696367,Learning Rate9.597653e-06\n",
      "Iter113,Testing Accuaracy0.8943,Training Accuracy0.88698184,Learning Rate9.117771e-06\n",
      "Iter114,Testing Accuaracy0.8944,Training Accuracy0.8870364,Learning Rate8.661882e-06\n",
      "Iter115,Testing Accuaracy0.8945,Training Accuracy0.88705456,Learning Rate8.228788e-06\n",
      "Iter116,Testing Accuaracy0.8945,Training Accuracy0.8871273,Learning Rate7.817348e-06\n",
      "Iter117,Testing Accuaracy0.8944,Training Accuracy0.8871273,Learning Rate7.426481e-06\n",
      "Iter118,Testing Accuaracy0.8944,Training Accuracy0.8871273,Learning Rate7.055157e-06\n",
      "Iter119,Testing Accuaracy0.8943,Training Accuracy0.8870909,Learning Rate6.702399e-06\n",
      "Iter120,Testing Accuaracy0.8946,Training Accuracy0.88698184,Learning Rate6.367279e-06\n",
      "Iter121,Testing Accuaracy0.8946,Training Accuracy0.8870182,Learning Rate6.048915e-06\n",
      "Iter122,Testing Accuaracy0.8945,Training Accuracy0.8870364,Learning Rate5.7464695e-06\n",
      "Iter123,Testing Accuaracy0.8945,Training Accuracy0.8870364,Learning Rate5.459146e-06\n",
      "Iter124,Testing Accuaracy0.8946,Training Accuracy0.88705456,Learning Rate5.1861884e-06\n",
      "Iter125,Testing Accuaracy0.8946,Training Accuracy0.88707274,Learning Rate4.9268792e-06\n",
      "Iter126,Testing Accuaracy0.8946,Training Accuracy0.8871273,Learning Rate4.6805353e-06\n",
      "Iter127,Testing Accuaracy0.8946,Training Accuracy0.8871273,Learning Rate4.4465087e-06\n",
      "Iter128,Testing Accuaracy0.8946,Training Accuracy0.8871091,Learning Rate4.224183e-06\n",
      "Iter129,Testing Accuaracy0.8946,Training Accuracy0.8871091,Learning Rate4.012974e-06\n",
      "Iter130,Testing Accuaracy0.8947,Training Accuracy0.8871273,Learning Rate3.8123253e-06\n",
      "Iter131,Testing Accuaracy0.8946,Training Accuracy0.8871091,Learning Rate3.6217089e-06\n",
      "Iter132,Testing Accuaracy0.8946,Training Accuracy0.88707274,Learning Rate3.4406235e-06\n",
      "Iter133,Testing Accuaracy0.8946,Training Accuracy0.88714546,Learning Rate3.2685923e-06\n",
      "Iter134,Testing Accuaracy0.8948,Training Accuracy0.88716364,Learning Rate3.1051627e-06\n",
      "Iter135,Testing Accuaracy0.8948,Training Accuracy0.88714546,Learning Rate2.9499047e-06\n",
      "Iter136,Testing Accuaracy0.8948,Training Accuracy0.88714546,Learning Rate2.8024094e-06\n",
      "Iter137,Testing Accuaracy0.8948,Training Accuracy0.88714546,Learning Rate2.6622888e-06\n",
      "Iter138,Testing Accuaracy0.8948,Training Accuracy0.8871818,Learning Rate2.5291745e-06\n",
      "Iter139,Testing Accuaracy0.8948,Training Accuracy0.8871818,Learning Rate2.4027156e-06\n",
      "Iter140,Testing Accuaracy0.8948,Training Accuracy0.8872,Learning Rate2.28258e-06\n",
      "Iter141,Testing Accuaracy0.8948,Training Accuracy0.8872182,Learning Rate2.168451e-06\n",
      "Iter142,Testing Accuaracy0.8948,Training Accuracy0.88723636,Learning Rate2.0600285e-06\n",
      "Iter143,Testing Accuaracy0.8949,Training Accuracy0.8872727,Learning Rate1.957027e-06\n",
      "Iter144,Testing Accuaracy0.8949,Training Accuracy0.88725454,Learning Rate1.8591757e-06\n",
      "Iter145,Testing Accuaracy0.8949,Training Accuracy0.88725454,Learning Rate1.7662169e-06\n",
      "Iter146,Testing Accuaracy0.8949,Training Accuracy0.88725454,Learning Rate1.677906e-06\n",
      "Iter147,Testing Accuaracy0.8949,Training Accuracy0.88725454,Learning Rate1.5940107e-06\n",
      "Iter148,Testing Accuaracy0.8949,Training Accuracy0.8872909,Learning Rate1.5143102e-06\n",
      "Iter149,Testing Accuaracy0.8949,Training Accuracy0.8872909,Learning Rate1.4385946e-06\n"
     ]
    }
   ],
   "source": [
    "#载入数据集\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\",one_hot=True)\n",
    "#运行次数\n",
    "max_steps = 150\n",
    "#图片数量\n",
    "image_num =3000\n",
    "#文件路径\n",
    "DIR=\"F:/yjs2019/Tensorflow_learn/\"\n",
    "#定义会话\n",
    "sess=tf.Session()\n",
    "#载入图片\n",
    "embedding = tf.Variable(tf.stack(mnist.test.images[:image_num]),trainable = False , name='embedding')\n",
    "# #每个批次的大小\n",
    "# batch_size = 100\n",
    "#计算一共有多少个批次\n",
    "# n_batch = mnist.train.num_examples // batch_size\n",
    "\n",
    "#参数概要\n",
    "def variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean',mean)#平均值\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev',stddev)#标准差\n",
    "        tf.summary.scalar('max',tf.reduce_max(var))#最大值\n",
    "        tf.summary.scalar('min',tf.reduce_min(var))#最小值\n",
    "        tf.summary.histogram('histogram',var)#直方图\n",
    "        \n",
    "#命名空间\n",
    "with tf.name_scope('input'):\n",
    "    #定义两个placeholder\n",
    "    x = tf.placeholder(tf.float32,[None,784],name = 'x-input')\n",
    "    y = tf.placeholder(tf.float32,[None,10],name = 'y-input')\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    lr = tf.Variable(0.001, dtype = tf.float32)\n",
    "#显示图片\n",
    "with tf.name_scope('input_reshape'):\n",
    "    image_shaped_input = tf.reshape(x,[-1,28,28,1])\n",
    "    tf.summary.image('input',image_shaped_input,10)\n",
    "with tf.name_scope('layer'):\n",
    "    with tf.name_scope('layer1'):\n",
    "        #创建一个简单的神经网络\n",
    "        with tf.name_scope('wights1'):\n",
    "            W1 = tf.Variable(tf.truncated_normal([784,500],stddev=0.1),name = 'W1')\n",
    "            variable_summaries(W1)\n",
    "        with tf.name_scope('biases'):\n",
    "            b1 = tf.Variable(tf.zeros([500])+0.1,name='b1')\n",
    "            variable_summaries(b1)\n",
    "        with tf.name_scope('tanh1'):\n",
    "            L1 = tf.nn.tanh(tf.matmul(x,W1)+b1)\n",
    "        with tf.name_scope('drop1'):\n",
    "            L1_drop = tf.nn.dropout(L1,keep_prob )\n",
    "        with tf.name_scope('wights2'):\n",
    "            W2 = tf.Variable(tf.truncated_normal([500,300],stddev=0.1),name='W2')\n",
    "            variable_summaries(W2)\n",
    "        with tf.name_scope('biases'):\n",
    "            b2 = tf.Variable(tf.zeros([300])+0.1,name='b2')\n",
    "            variable_summaries(b2)\n",
    "        with tf.name_scope('tanh2'):\n",
    "            L2 = tf.nn.tanh(tf.matmul(L1_drop,W2)+b2)\n",
    "        with tf.name_scope('drop2'):\n",
    "            L2_drop = tf.nn.dropout(L2,keep_prob )\n",
    "        with tf.name_scope('wights3'):\n",
    "            W3 = tf.Variable(tf.truncated_normal([300,10],stddev=0.1),name='W3')\n",
    "            variable_summaries(W2)\n",
    "        with tf.name_scope('biases3'):\n",
    "            b3 = tf.Variable(tf.zeros([10])+0.1,name='b3')\n",
    "            variable_summaries(b3)\n",
    "        with tf.name_scope('tanh'):\n",
    "            prediction = tf.nn.tanh(tf.matmul(L2_drop,W3)+b3)\n",
    "\n",
    "#二次代价函数\n",
    "with tf.name_scope('loss'):\n",
    "# loss = tf.reduce_mean(tf.square(y-prediction))\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y , logits = prediction))\n",
    "    tf.summary.scalar('loss',loss)\n",
    "with tf.name_scope('train'):\n",
    "    #使用梯度下降法\n",
    "#     train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n",
    "    train_step = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "\n",
    "#初始化变量\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "    with tf.name_scope('correct_prediction'):\n",
    "        #结果存放在一个布尔型列表中\n",
    "        correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1))#argmax返回一维张量中最大值\n",
    "        #求准确率\n",
    "    with tf.name_scope('accuracy'):\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "        tf.summary.scalar('accuracy',accuracy)\n",
    "\n",
    "#产生metadata文件\n",
    "if tf.gfile.Exists(DIR+'projector/projector/metadata.tsv'):\n",
    "    tf.gfile.DeleteRecursively(DIR+'projector/projector/metadata.tsv')\n",
    "with open(DIR+'projector/projector/metadata.tsv','w') as f:\n",
    "    labels=sess.run(tf.argmax(mnist.test.labels[:],1))\n",
    "    for i in range(image_num):\n",
    "        f.write(str(labels[i])+'\\n')\n",
    "#合并所有的summary\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "projector_writer = tf.summary.FileWriter(DIR+'projector/projector',sess.graph)\n",
    "saver = tf.train.Saver()\n",
    "config = projector.ProjectorConfig()\n",
    "embed = config.embeddings.add()\n",
    "embed.tensor_name = embedding.name\n",
    "embed.metadata_path = DIR+'projector/projector/metadata.tsv'\n",
    "embed.sprite.image_path = DIR+'projector/data/mnist_10k_sprite.png'\n",
    "embed.sprite.single_image_dim.extend([28,28])\n",
    "projector.visualize_embeddings(projector_writer,config)\n",
    "\n",
    "for epoch in range(max_steps):\n",
    "    #每个批次100个样本\n",
    "    sess.run(tf.assign(lr,0.003 * (0.95 ** epoch)))\n",
    "    batch_xs,batch_ys = mnist.train.next_batch(100)\n",
    "    run_options = tf.RunOptions(trace_level = tf.RunOptions.FULL_TRACE)\n",
    "    run_metadata = tf.RunMetadata()\n",
    "#     sess.run(train_step,feed_dict={x:batch_xs,y:batch_ys},options=run_options,run_metadata=run_metadata)\n",
    "#     summary = sess.run(merged,feed_dict={x:batch_xs,y:batch_ys},options=run_options,run_metadata=run_metadata)\n",
    "    summary,_=sess.run([merged,train_step],feed_dict = {x:batch_xs,y:batch_ys,keep_prob:1.0},options = run_options,run_metadata=run_metadata)\n",
    "    projector_writer.add_run_metadata(run_metadata,'step%3d'%epoch)\n",
    "    projector_writer.add_summary(summary, epoch)\n",
    "    learning_rate = sess.run(lr)\n",
    "    test_acc = sess.run(accuracy,feed_dict = {x:mnist.test.images,y:mnist.test.labels,keep_prob:1.0})\n",
    "    train_acc = sess.run(accuracy,feed_dict = {x:mnist.train.images,y:mnist.train.labels,keep_prob:1.0})\n",
    "    if epoch%1 == 0:\n",
    "        print(\"Iter\" + str(epoch)+\",Testing Accuaracy\"+str(test_acc)+\",Training Accuracy\"+str(train_acc)+\",Learning Rate\" +str(learning_rate))\n",
    "saver.save(sess,DIR+'projector/projector/a_model.ckpt',global_step = max_steps)\n",
    "projector_writer.close()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.6tensorflow",
   "language": "python",
   "name": "py3.6tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
